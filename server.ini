[general]
# Public address used by clients
public_address = 127.0.0.1:6379

# Private address. Used internally for communicating between SableDB nodes
private_address = 127.0.0.1:7379

# If set, SableDB stores its cluster & shard state in another instance of SableDB and advanced replication
# features such as "auto-failover" are enabled.
# The address should match the cluster DB *public_address*
#
# NOTE about TLS: if your cluster database uses TLS, use the format: tls://<IP>:<PORT>
#cluster_address = 127.0.0.1:6379

# Server workers count. If value is 0, workers count is set to `(number of CPUs / 2)`
workers = 0

# Log verbosity (can be one of: info, warn, error, trace, debug)
log_level = info

# Log directory, if commented or left empty, use standard output
logdir = ""

# Path to the storage directory
# On Linux, and in order to improve performance - at the cost of durability - you can place it on shared memory,
# e.g. `/dev/shm/sabledb.db`
db_path = "sabledb.db"

# path to locate / write configuration files
config_dir = "."

# To enable TLS based communication, set here the path to the certificate + the key
# cert = ssl/sabledb.crt
# key = ssl/sabledb.key

[cron]
# We evict orphan records every N seconds
# An orphan record is a record which is no longer accessible by the user (e.g. an hash field that the parent
# hash key was overridden by a string key)
evict_orphan_records_secs = 3600

# Should the cron thread run "compaction" after a successful eviction?
compaction_after_eviction = false

# Scan the database for statistics purposes every `scan_keys_secs` seconds
scan_keys_secs = 30

# Delete is O(1) for simple key (e.g. string key). However, for types with children (e.g. hash)
# the deletion process requires O(N+1) where N is the number of children. Some types, (e.g. sorted set)
# require O(2xN + 1). By setting this flag to `true`, SableDB will only delete the primary key (i.e. the
# key is no longer accessible by the user), the remaining children will be deleted by a background thread
instant_delete = true

# In a cluster configuration or as part of a replication group, this node will update its status in the cluster database
# every N milliseconds.
cluster_database_updates_interval_ms = 500

# The cron job is set to activate every N milliseconds to complete its tasks.
cron_interval_ms = 100

[client_limits]
# Build up to `response_buffer_size` bytes in memory before flushing
# to the network
client_response_buffer_size = 1MB

[replication_limits]
# Limit the size of a single replication update message
# in memory before sending it over the network
single_update_buffer_size = 50MB

# A single replication response can hold up to `num_updates_per_message`
# database updates
num_updates_per_message = 1M

# If there are changes queued for replication, they are sent immediately.
# However, when there are no changes to send to the replica, the replication task
# suspend itself for `check_for_updates_interval_ms` milliseconds.
check_for_updates_interval_ms = 50

[rocksdb]
# If set to true, writes will not first be recorded in the Write-Ahead Log (WAL). This can lead to significant 
# performance improvements. However, it is important to note that enabling this setting increases the risk of data loss 
# in the event of a crash, as the changes will not be fully secured.
disable_wal = false

# The pipelined write feature added in RocksDB 5.5 is to improve concurrent write throughput in case WAL is enabled.
# By default, a single write thread queue is maintained for concurrent writers. The thread gets to the head of the
# queue becomes write batch group leader and responsible for writing to WAL and memtable for the batch group.
# One observation is that WAL writes and memtable writes are sequential, and by making them run in parallel we can
# increase throughput.
# Default: true
enable_pipelined_write = true

# If you're doing point lookups on an uncompacted DB, you definitely want to turn bloom filters on. RocksDB uses
# bloom filters to avoid unnecessary disk reads. Default `bloom_filter_bits_per_key` is 10, which yields `~1%` false positive rate.
# Larger `bloom_filter_bits_per_key` values will reduce false positive rate, but increase memory usage and space amplification.
# To disable bloom filter, set this value to `0`
# Default: 10
bloom_filter_bits_per_key = 10

# Enable data compression using Snappy
compression_enabled = true

# Each write goes through a memtable which is backed by a WAL file.
# Once the memtable is full, it is marked as "immutable" and a new
# memtable is created. This directive sets the size of the memtable
write_buffer_size = 50MB

# Number of IO threads available for RocksDB to perform flush & compaction
max_background_jobs = 8

# Sets the maximum number of write buffers that are built up in memory.
# The default is 4, so that when 1 write buffer
# is being flushed to storage, new writes can continue to the other
# write buffer.
max_write_buffer_number = 4

# If "wal_ttl_seconds" is not 0, then
# WAL files will be checked every `wal_ttl_seconds / 2` and those that
# are older than `wal_ttl_seconds` will be deleted.
wal_ttl_seconds = 3600

# By default, RocksDB check whether WAL flush is needed after each write
# This option disables this and moves it to a "time based" WAL flush
#
# This is a good tradeoff between persistency and performance (if you are
# willing to lose data that were not flushed to disk in the last
# `manual_wal_flush_interval_ms` milliseconds.)
manual_wal_flush = true

# If `manual_wal_flush` is true, SableDB will flush the wal every `N` milliseconds
manual_wal_flush_interval_ms = 250

# Sets the number of open files that can be used by the DB. You may need to
# increase this if your database has a large working set. Value `-1` means
# files opened are always kept open. You can estimate number of files based
# on target_file_size_base and target_file_size_multiplier for level-based
# compaction. For universal-style compaction, you can usually set it to `-1`.
max_open_files = -1

# Sets the minimum number of write buffers that will be merged together
# before writing to storage.  If set to `1`, then
# all write buffers are flushed to L0 as individual files and this increases
# read amplification because a get request has to check in all of these
# files. Also, an in-memory merge may result in writing lesser
# data to storage if there are duplicate records in each of these
# individual write buffers.
#
# Default: 1
min_write_buffer_number_to_merge = 1

# Allow the OS to mmap file for reading sst tables.
#
# Default: false
allow_mmap_reads = false

# Allow the OS to mmap file for writing.
#
# Default: false
allow_mmap_writes = false
