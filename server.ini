[general]
# Public address used by clients
public_address = 127.0.0.1:6379

# Private address. Used internally for communicating between SableDB nodes
private_address = 127.0.0.1:7379

# If set, SableDB stores its cluster & shard state in another instance of SableDB and advanced replication
# features such as "auto-failover" are enabled.
# The address should match the cluster DB *public_address*
#
# NOTE about TLS: if your cluster database uses TLS, use the format: tls://<IP>:<PORT>
#cluster_address = 127.0.0.1:6379

# Server workers count. set to 0 to let sabledb decide which defaults to `(number of CPUs / 2)`
workers = 0

# Log verbosity (can be one of: info, warn, error, trace, debug)
log_level = info

# Log directory
logdir = "logs/"

# Path to the storage directory
# On Linux, and in order to improve performance - at the cost of durability - you can place it on shared memory,
# e.g. `/dev/shm/sabledb.db`
db_path = "sabledb.db"

# path to locate / write configuration files
config_dir = "."

# To enable TLS based communication, set here the path to the certificate + the key
# cert = ssl/sabledb.crt
# key = ssl/sabledb.key

[cron]
# We evict orphan records every N seconds
# An orphan record is a record which is no longer accessible by the user (e.g. an hash field that the parent
# hash key was overridden by a string key)
evict_orphan_records_secs = 3600

# Scan the database for statistics purposes every `scan_keys_secs` seconds
scan_keys_secs = 30

# Delete is O(1) for simple key (e.g. string key). However, for types with children (e.g. hash)
# the deletion process requires O(N+1) where N is the number of children. Some types, (e.g. sorted set)
# require O(2xN + 1). By setting this flag to `true`, SableDb will only delete the primary key (i.e. the
# is no longer accessible by the user), the remaining type children will be deleted by the eviction task
# using a background thread
instant_delete = true

[client_limits]
# Build up to `response_buffer_size` bytes in memory before flushing
# to the network
client_response_buffer_size = 1MB

[replication_limits]
# Limit the size of a single replication update message
# in memory before sending it over the network
single_update_buffer_size = 50MB

# A single replication response can hold up to `num_updates_per_message`
# database updates
num_updates_per_message = 1M

# If there are changes queued for replication, they are sent immediately.
# However, when there are no changes to send to the replica, the replication task
# suspend itself for `check_for_updates_interval_ms` milliseconds.
check_for_updates_interval_ms = 50

[rocksdb]
# If true, writes will not first go to the write ahead log ("WAL"),
# and the write may get lost after a crash.
# IMPORTANT: setting this value to `true` will get you performance
# improvement but will also cause to lose of data in case of crash
disable_wal = false

# The pipelined write feature added in RocksDB 5.5 is to improve concurrent write throughput in case WAL is enabled.
# By default, a single write thread queue is maintained for concurrent writers. The thread gets to the head of the
# queue becomes write batch group leader and responsible for writing to WAL and memtable for the batch group.
# One observation is that WAL writes and memtable writes are sequential, and by making them run in parallel we can
# increase throughput.
# Default: true
enable_pipelined_write = true

# If you're doing point lookups on an uncompacted DB, you definitely want to turn bloom filters on. RocksDb uses
# bloom filters to avoid unnecessary disk reads. Default `bloom_filter_bits_per_key` is 10, which yields `~1%` false positive rate.
# Larger `bloom_filter_bits_per_key` values will reduce false positive rate, but increase memory usage and space amplification.
# To disable bloom filter, set this value to `0`
# Default: 10
bloom_filter_bits_per_key = 10

# Enable data compression using Snappy
compression_enabled = true

# Each write goes through a memtable which is backed by a WAL file.
# Once the memtable is full, it is marked as "immutable" and a new
# memtable is created. This directive sets the size of the memtable
write_buffer_size = 50MB

# Number of IO threads available for RocksDb to perform flush & compaction
max_background_jobs = 8

# Sets the maximum number of write buffers that are built up in memory.
# The default is 4, so that when 1 write buffer
# is being flushed to storage, new writes can continue to the other
# write buffer.
max_write_buffer_number = 4

# If "wal_ttl_seconds" is not 0, then
# WAL files will be checked every `wal_ttl_seconds / 2` and those that
# are older than `wal_ttl_seconds` will be deleted.
wal_ttl_seconds = 3600

# By default, RocksDb check whether WAL flush is needed after each write
# This option disables this and moves it to a "time based" WAL flush
#
# This is a good tradeoff between persistency and performance (if you are
# willing to lose data that were not flushed to disk in the last
# `manual_wal_flush_interval_ms` milliseconds.)
manual_wal_flush = true

# If `manual_wal_flush` is true, SableDb will flush the wal every `N` milliseconds
manual_wal_flush_interval_ms = 500

# Sets the number of open files that can be used by the DB. You may need to
# increase this if your database has a large working set. Value `-1` means
# files opened are always kept open. You can estimate number of files based
# on target_file_size_base and target_file_size_multiplier for level-based
# compaction. For universal-style compaction, you can usually set it to `-1`.
max_open_files = -1
